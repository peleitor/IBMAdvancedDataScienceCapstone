{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import analyticslib \nfrom analyticslib import DataSet"
        }, 
        {
            "source": "# Load Data\n- Load data from previous Data Cleansing stage\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "total 864\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 11505 Mar 10 14:37 myfile.py\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 16 16:13 test.csv\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 14856 Mar 26 20:58 analyticslib.py\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 26 20:59 __pycache__\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 30 09:54 cleaned_data.csv\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 30 12:55 created_features_p.parquet.20190330\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 30 13:11 cleaned_data_p.parquet\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 30 14:22 created_features_p.parquet\r\ndrwx------. 2 sa7d-5e452b87283280-4ab9c5f515a1 users  4096 Mar 30 18:18 created_features_p_balanced.parquet\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 87592 Mar 30 19:01 best_model_1.h5\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 76360 Mar 30 20:27 best_model_iter_32_80.h5\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 28312 Mar 30 22:06 best_model.h5\r\n-rw-------. 1 sa7d-5e452b87283280-4ab9c5f515a1 users 51510 Mar 30 22:22 best_model_capstone.tgz\r\n"
                }
            ], 
            "source": "!ls -lrt"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "loading local parquet\n"
                }
            ], 
            "source": "#dataMain = DataSet(spark_context=sc, filename=\"created_features_p.parquet\", file_storage=\"local_parquet\", sampling_mode=\"ALL\")\ndataMain = DataSet(spark_context=sc, filename=\"created_features_p_balanced.parquet\", file_storage=\"local_parquet\", sampling_mode=\"ALL\")\n"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "11742"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dataMain.dataset.count()"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+-----+\n|         PolyPhen|count|\n+-----------------+-----+\n|           benign| 3861|\n|possibly_damaging| 3932|\n|probably_damaging| 3949|\n+-----------------+-----+\n\n"
                }
            ], 
            "source": "dataMain.dataset.groupBy(\"PolyPhen\").count().show()"
        }, 
        {
            "source": "# Prepare data for MLLib models\n- At this point:\n   - Attributes have been selected\n   - Attribute vectors have been assembled\n   - Data has been normalized\n- Now we will define steps for training and evaluating a model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Dataset count: 11742\nDataset train count: 10581\n+-----------------+-----+\n|         PolyPhen|count|\n+-----------------+-----+\n|           benign| 3463|\n|possibly_damaging| 3539|\n|probably_damaging| 3536|\n+-----------------+-----+\n\nDataset test count: 1204\n+-----------------+-----+\n|         PolyPhen|count|\n+-----------------+-----+\n|           benign|  400|\n|possibly_damaging|  387|\n|probably_damaging|  374|\n+-----------------+-----+\n\n"
                }
            ], 
            "source": "df = dataMain.dataset\n\nsplits = df.randomSplit([0.9, 0.1])\ndf_train = splits[0]\ndf_test = splits[1]\n\nprint(\"Dataset count: %d\" % (df.count()) )\n\nprint(\"Dataset train count: %d\" % (df_train.count()) )\ndf_train.groupBy(\"PolyPhen\").count().show()\n\nprint(\"Dataset test count: %d\" % (df_test.count()) )\ndf_test.groupBy(\"PolyPhen\").count().show()\n"
        }, 
        {
            "source": "## MLLib Models", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol=\"PolyPhen\", outputCol=\"label\")\n\n\nvectorAssembler = VectorAssembler(inputCols=[\"cadd_scores_vec\", \"freqs_vec\", \"CLASS\",\"STRAND\",\"LoFtool\",\"BLOSUM62_norm\",\"ORIGIN_LOG2\", \"CHROMVec\", \"IMPACTVec\", \"CLNVCVec\", \"SIFTVec\"], outputCol=\"features\")\n#vectorAssembler = VectorAssembler(inputCols=[\"cadd_scores_vec\"], outputCol=\"features\")"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5)\n\n#from pyspark.ml.classification import MultilayerPerceptronClassifier\n#layers = [5, 5, 4]\n#mlp_trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n\n\n#from pyspark.ml.classification import GBTClassifier\n#gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\n\npipeline_dt = Pipeline(stages=[indexer, vectorAssembler, dt])\npipeline_rf = Pipeline(stages=[indexer, vectorAssembler, rf])\n"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#pipeline_mlp = Pipeline(stages=[indexer, vectorAssembler, mlp_trainer])\n"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cadd_scores_vec</th>\n      <th>freqs_vec</th>\n      <th>CLASS</th>\n      <th>STRAND</th>\n      <th>LoFtool</th>\n      <th>BLOSUM62_norm</th>\n      <th>ORIGIN_LOG2</th>\n      <th>CHROMVec</th>\n      <th>IMPACTVec</th>\n      <th>CLNVCVec</th>\n      <th>SIFTVec</th>\n      <th>PolyPhen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.001, -4.314148]</td>\n      <td>[0.0, 1e-05, 0.0]</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0.0243</td>\n      <td>[1.0]</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>(1.0, 0.0, 0.0)</td>\n      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n      <td>(0.0, 0.0, 1.0, 0.0)</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.001, -3.413448]</td>\n      <td>[0.0055, 0.00296, 0.0078]</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0.5310</td>\n      <td>[-1.0]</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>(1.0, 0.0, 0.0)</td>\n      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n      <td>(0.0, 0.0, 1.0, 0.0)</td>\n      <td>benign</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "      cadd_scores_vec                  freqs_vec  CLASS  STRAND  LoFtool  \\\n0  [0.001, -4.314148]          [0.0, 1e-05, 0.0]      0      -1   0.0243   \n1  [0.001, -3.413448]  [0.0055, 0.00296, 0.0078]      0      -1   0.5310   \n\n  BLOSUM62_norm  ORIGIN_LOG2  \\\n0         [1.0]          0.0   \n1        [-1.0]          0.0   \n\n                                            CHROMVec        IMPACTVec  \\\n0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0)   \n1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0)   \n\n                         CLNVCVec               SIFTVec PolyPhen  \n0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)  (0.0, 0.0, 1.0, 0.0)   benign  \n1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)  (0.0, 0.0, 1.0, 0.0)   benign  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_train.limit(2).toPandas()"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_dt = pipeline_dt.fit(df_train)\nmodel_rf = pipeline_rf.fit(df_train)\n"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#model_mlp = pipeline_mlp.fit(df_train)"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction_dt = model_dt.transform(df_test)\nprediction_rf = model_rf.transform(df_test)\n"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#prediction_mlp = model_mlp.transform(df_train)\n"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "decision tree:\n+----------+-----+--------------------+\n|prediction|label|            features|\n+----------+-----+--------------------+\n|       2.0|  2.0|(46,[0,1,2,3,4,5,...|\n|       2.0|  2.0|(46,[0,1,6,7,8,10...|\n|       2.0|  2.0|(46,[0,1,2,4,6,7,...|\n|       2.0|  2.0|(46,[0,1,6,7,8,21...|\n|       2.0|  2.0|(46,[0,1,6,7,8,9,...|\n+----------+-----+--------------------+\nonly showing top 5 rows\n\nrandom forest:\n+----------+-----+--------------------+\n|prediction|label|            features|\n+----------+-----+--------------------+\n|       2.0|  2.0|(46,[0,1,2,3,4,5,...|\n|       2.0|  2.0|(46,[0,1,6,7,8,10...|\n|       2.0|  2.0|(46,[0,1,2,4,6,7,...|\n|       2.0|  2.0|(46,[0,1,6,7,8,21...|\n|       2.0|  2.0|(46,[0,1,6,7,8,9,...|\n+----------+-----+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "# Select example rows to display.\nprint(\"decision tree:\")\nprediction_dt.select(\"prediction\", \"label\", \"features\").show(5)\nprint(\"random forest:\")\nprediction_rf.select(\"prediction\", \"label\", \"features\").show(5)\n#print(\"multi layer perceptron:\")\n#prediction_mlp.select(\"*\").show(5)\n"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "DT - accuracy = 0.628738 \nRF - accuracy = 0.638704 \n"
                }
            ], 
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(prediction_dt)\nprint(\"DT - accuracy = %g \" % (accuracy))\n\n\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(prediction_rf)\nprint(\"RF - accuracy = %g \" % (accuracy))\n\n\n\n\n# Select (prediction, true label) and compute test error\n#evaluator = MulticlassClassificationEvaluator(\n#    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n#accuracy = evaluator.evaluate(prediction_mlp)\n#print(\"MLP - Test Error = %g \" % (1.0 - accuracy))\n\n"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+--------------------+-----+------+-------+-------------+-----------+---------------+-------------+-------------+-------------+-----------------+-----+--------------------+--------------------+--------------------+----------+\n|  cadd_scores_vec|           freqs_vec|CLASS|STRAND|LoFtool|BLOSUM62_norm|ORIGIN_LOG2|       CHROMVec|    IMPACTVec|     CLNVCVec|      SIFTVec|         PolyPhen|label|            features|       rawPrediction|         probability|prediction|\n+-----------------+--------------------+-----+------+-------+-------------+-----------+---------------+-------------+-------------+-------------+-----------------+-----+--------------------+--------------------+--------------------+----------+\n|[0.001,-2.114737]|[0.0012,0.00311,0...|    1|     1|  0.077|        [1.0]|        0.0|(23,[13],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[3],[1.0])|           benign|  2.0|(46,[0,1,2,3,4,5,...|[0.74635849170433...|[0.14927169834086...|       2.0|\n|[0.001,-2.002075]|           (3,[],[])|    0|     1| 0.0212|       [-1.0]|        0.0| (23,[0],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,10...|[1.08276562928337...|[0.21655312585667...|       2.0|\n|[0.002,-1.650536]|  [0.492,0.0,0.4816]|    0|    -1|  0.659|       [-1.0]|        0.0| (23,[9],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,4,6,7,...|[0.91339398028358...|[0.18267879605671...|       2.0|\n| [0.002,-1.57751]|           (3,[],[])|    0|     1|0.00165|        [1.0]|        0.0|(23,[11],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,21...|[0.98855927219130...|[0.19771185443826...|       2.0|\n|[0.004,-1.348103]|           (3,[],[])|    0|    -1|  0.531|       [-1.0]|       -1.0| (23,[9],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,9,...|[0.72481648527353...|[0.14496329705470...|       2.0|\n| [0.01,-1.094352]|[6.0E-4,0.0067,0....|    0|     1|  0.601|       [-1.0]|        0.0|(23,[16],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,4,6,...|[0.54171825034908...|[0.10834365006981...|       2.0|\n|[0.012,-1.058576]| [2.0E-4,1.1E-4,0.0]|    0|    -1|  0.175|        [1.0]|        0.0| (23,[7],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,6,7,...|[0.89095037625564...|[0.17819007525112...|       2.0|\n|[0.012,-1.055062]|   [0.0,0.00121,0.0]|    0|    -1|  0.157|        [0.0]|       -1.0|(23,[11],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[3],[1.0])|possibly_damaging|  0.0|(46,[0,1,3,6,7,9,...|[0.91487540808827...|[0.18297508161765...|       2.0|\n|[0.014,-1.027394]|           (3,[],[])|    0|     1|0.00386|        [1.0]|        0.0| (23,[6],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,16...|[0.98855927219130...|[0.19771185443826...|       2.0|\n|[0.027,-0.888072]|[0.4065,0.45194,0...|    0|    -1|  0.714|        [0.0]|       -1.0| (23,[2],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,4,6,...|[0.45891513092425...|[0.09178302618485...|       2.0|\n|[0.028,-0.882654]|  [0.0151,0.0,0.015]|    0|    -1|  0.984|        [0.0]|        0.0| (23,[3],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,4,6,7,...|[0.79867735192203...|[0.15973547038440...|       2.0|\n|[0.042,-0.802332]|           (3,[],[])|    0|    -1|  0.803|        [1.0]|        0.0| (23,[3],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[3],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,13...|[0.82869554872402...|[0.16573910974480...|       2.0|\n|[0.046,-0.784359]|[0.0447,0.01264,0...|    0|    -1| 0.0151|       [-1.0]|       -1.0| (23,[0],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,4,6,...|[0.53737753957331...|[0.10747550791466...|       2.0|\n|[0.048,-0.777383]|           (3,[],[])|    0|    -1| 0.0243|        [1.0]|        0.0| (23,[3],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[1],[1.0])|possibly_damaging|  0.0|(46,[0,1,6,7,8,13...|[1.58676771368831...|[0.31735354273766...|       2.0|\n| [0.05,-0.770387]| [2.0E-4,2.3E-4,0.0]|    0|     1|  0.566|        [1.0]|        0.0|(23,[12],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,6,7,...|[0.68407990319403...|[0.13681598063880...|       2.0|\n|[0.065,-0.720491]|[0.3731,0.40195,0...|    0|    -1|  0.827|        [1.0]|       -1.0| (23,[9],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,2,3,4,6,...|[0.26612058369932...|[0.05322411673986...|       2.0|\n|[0.069,-0.707431]|           (3,[],[])|    0|     1| 0.0212|        [1.0]|        0.0| (23,[0],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,10...|[0.98855927219130...|[0.19771185443826...|       2.0|\n|[0.072,-0.699644]| [0.0,6.0E-5,2.0E-4]|    0|     1| 0.0466|        [1.0]|        0.0|(23,[11],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,3,4,6,7,...|[0.98855927219130...|[0.19771185443826...|       2.0|\n|[0.074,-0.694801]| [0.0,1.3E-4,8.0E-4]|    1|     1|6.89E-5|        [1.0]|       -1.0| (23,[6],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[3],[1.0])|           benign|  2.0|(46,[0,1,3,4,5,6,...|[1.61340768935705...|[0.32268153787141...|       2.0|\n|[0.078,-0.685096]|           (3,[],[])|    0|     1|  0.157|        [1.0]|        0.0|(23,[12],[1.0])|(3,[0],[1.0])|(6,[0],[1.0])|(4,[2],[1.0])|           benign|  2.0|(46,[0,1,6,7,8,22...|[0.89095037625564...|[0.17819007525112...|       2.0|\n+-----------------+--------------------+-----+------+-------+-------------+-----------+---------------+-------------+-------------+-------------+-----------------+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "prediction_rf.select(\"*\").show()"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.metrics import classification_report\nimport numpy as np\n"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Classification report for Decision Tree\n             precision    recall  f1-score   support\n\n        0.0       0.47      0.52      0.49       393\n        1.0       0.67      0.61      0.64       413\n        2.0       0.75      0.76      0.75       398\n\navg / total       0.63      0.63      0.63      1204\n\nClassification report for Random Forest\n             precision    recall  f1-score   support\n\n        0.0       0.49      0.41      0.45       393\n        1.0       0.65      0.77      0.70       413\n        2.0       0.75      0.73      0.74       398\n\navg / total       0.63      0.64      0.63      1204\n\n"
                }
            ], 
            "source": "for df_result, model_type in [(prediction_dt, \"Decision Tree\"), (prediction_rf, \"Random Forest\")]:\n    y_pred = df_result.select(\"prediction\").collect()\n    y_lab  = df_result.select(\"label\").collect()\n\n    yp = list( map( lambda r: r[0], y_pred ))\n    yl = list( map( lambda r: r[0], y_lab ))\n\n    print(\"Classification report for %s\" % (model_type))\n    print(classification_report(yl, yp))"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 21, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1204"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "len(y_pred)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "source": "## Prepare data for Deep NN model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.linalg import SparseVector, DenseVector\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\n\ndef dense_to_array(v):\n  new_array = list([float(x) for x in v])\n  return new_array\n\ndef sparse_to_array(v):\n  v = DenseVector(v)\n  new_array = list([float(x) for x in v])\n  return new_array\n\ndense_to_array_udf = F.udf(dense_to_array, T.ArrayType(T.FloatType()))\n\nsparse_to_array_udf = F.udf(sparse_to_array, T.ArrayType(T.FloatType()))\n"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10538 1204\n"
                }
            ], 
            "source": "df_keras_src_train = df_train\ndf_keras_src_test = df_test\nprint(df_train.count(), df_test.count())\n\n"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NOTE.- For some reason, the UDF fails unless the data set is brought to one single node (coalesce(1))\n\ndf_keras_x_train = df_keras_src_train.coalesce(1) \\\n    .withColumn(\"cadd_a\", dense_to_array_udf('cadd_scores_vec')) \\\n    .withColumn(\"freqs_a\", dense_to_array_udf('freqs_vec')) \\\n    .withColumn(\"CHROM_a\", dense_to_array_udf('CHROMVec')) \\\n    .withColumn(\"IMPACT_a\", dense_to_array_udf('IMPACTVec')) \\\n    .withColumn(\"CLNVCV_a\", dense_to_array_udf('CLNVCVec')) \\\n    .withColumn(\"SIFT_a\", dense_to_array_udf('SIFTVec')) \\\n    .withColumn(\"BLOSUM62_a\", dense_to_array_udf('BLOSUM62_norm')) \\\n    .drop('cadd_scores_vec') \\\n    .drop('freqs_vec') \\\n    .drop('CHROMVec') \\\n    .drop('IMPACTVec') \\\n    .drop('CLNVCVec') \\\n    .drop('SIFTVec') \\\n    .select(\"*\")\n\ndf_keras_x_test = df_keras_src_test.coalesce(1) \\\n    .withColumn(\"cadd_a\", dense_to_array_udf('cadd_scores_vec')) \\\n    .withColumn(\"freqs_a\", dense_to_array_udf('freqs_vec')) \\\n    .withColumn(\"CHROM_a\", dense_to_array_udf('CHROMVec')) \\\n    .withColumn(\"IMPACT_a\", dense_to_array_udf('IMPACTVec')) \\\n    .withColumn(\"CLNVCV_a\", dense_to_array_udf('CLNVCVec')) \\\n    .withColumn(\"SIFT_a\", dense_to_array_udf('SIFTVec')) \\\n    .withColumn(\"BLOSUM62_a\", dense_to_array_udf('BLOSUM62_norm')) \\\n    .drop('cadd_scores_vec') \\\n    .drop('freqs_vec') \\\n    .drop('CHROMVec') \\\n    .drop('IMPACTVec') \\\n    .drop('CLNVCVec') \\\n    .drop('SIFTVec') \\\n    .select(\"*\")\n"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10538 1204\n"
                }
            ], 
            "source": "print(df_keras_x_train.count(), df_keras_x_test.count())\n\n"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- CLASS: integer (nullable = true)\n |-- STRAND: integer (nullable = true)\n |-- LoFtool: double (nullable = true)\n |-- BLOSUM62_norm: vector (nullable = true)\n |-- ORIGIN_LOG2: double (nullable = true)\n |-- PolyPhen: string (nullable = true)\n |-- cadd_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- freqs_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- CHROM_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- IMPACT_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- CLNVCV_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- SIFT_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n |-- BLOSUM62_a: array (nullable = true)\n |    |-- element: float (containsNull = true)\n\n"
                }
            ], 
            "source": "df_keras_x_train.printSchema()"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_keras_prep_train = df_keras_x_train.select(\"CLASS\", \"STRAND\", \"LoFtool\", \"ORIGIN_LOG2\", \\\n            \"cadd_a\", df_keras_x_train.cadd_a.getItem(0), df_keras_x_train.cadd_a.getItem(1), \\\n            df_keras_x_train.freqs_a.getItem(0), df_keras_x_train.freqs_a.getItem(1), df_keras_x_train.freqs_a.getItem(2), \\\n            df_keras_x_train.CHROM_a.getItem(0), df_keras_x_train.CHROM_a.getItem(1), df_keras_x_train.CHROM_a.getItem(2), df_keras_x_train.CHROM_a.getItem(3), df_keras_x_train.CHROM_a.getItem(4), df_keras_x_train.CHROM_a.getItem(5),  \\\n            df_keras_x_train.CHROM_a.getItem(6), df_keras_x_train.CHROM_a.getItem(7), df_keras_x_train.CHROM_a.getItem(8), df_keras_x_train.CHROM_a.getItem(9), df_keras_x_train.CHROM_a.getItem(10), df_keras_x_train.CHROM_a.getItem(11),  \\\n            df_keras_x_train.CHROM_a.getItem(12), df_keras_x_train.CHROM_a.getItem(13), df_keras_x_train.CHROM_a.getItem(14), df_keras_x_train.CHROM_a.getItem(15), df_keras_x_train.CHROM_a.getItem(16), df_keras_x_train.CHROM_a.getItem(17), \\\n            df_keras_x_train.CHROM_a.getItem(18), df_keras_x_train.CHROM_a.getItem(19), df_keras_x_train.CHROM_a.getItem(20), df_keras_x_train.CHROM_a.getItem(21), df_keras_x_train.CHROM_a.getItem(22), \\\n            df_keras_x_train.IMPACT_a.getItem(0), df_keras_x_train.IMPACT_a.getItem(1), df_keras_x_train.IMPACT_a.getItem(2),  \\\n            df_keras_x_train.CLNVCV_a.getItem(0), df_keras_x_train.CLNVCV_a.getItem(1), df_keras_x_train.CLNVCV_a.getItem(2), df_keras_x_train.CLNVCV_a.getItem(3), df_keras_x_train.CLNVCV_a.getItem(4), df_keras_x_train.CLNVCV_a.getItem(4),  \\\n            df_keras_x_train.SIFT_a.getItem(0), df_keras_x_train.SIFT_a.getItem(1), df_keras_x_train.SIFT_a.getItem(2), df_keras_x_train.SIFT_a.getItem(3),  \\\n            df_keras_x_train.BLOSUM62_a.getItem(0),\n            \"PolyPhen\").drop(\"cadd_a\").drop(\"freqs_a\").drop(\"CHROM_a\").drop(\"IMPACT_a\").drop(\"CLNVCV_a\").drop(\"SIFT_a\").drop(\"BLOSUM62_norm\")\n\ndf_keras_prep_test = df_keras_x_test.select(\"CLASS\", \"STRAND\", \"LoFtool\", \"ORIGIN_LOG2\", \\\n            \"cadd_a\", df_keras_x_test.cadd_a.getItem(0), df_keras_x_test.cadd_a.getItem(1), \\\n            df_keras_x_test.freqs_a.getItem(0), df_keras_x_test.freqs_a.getItem(1), df_keras_x_test.freqs_a.getItem(2), \\\n            df_keras_x_test.CHROM_a.getItem(0), df_keras_x_test.CHROM_a.getItem(1), df_keras_x_test.CHROM_a.getItem(2), df_keras_x_test.CHROM_a.getItem(3), df_keras_x_test.CHROM_a.getItem(4), df_keras_x_test.CHROM_a.getItem(5),  \\\n            df_keras_x_test.CHROM_a.getItem(6), df_keras_x_test.CHROM_a.getItem(7), df_keras_x_test.CHROM_a.getItem(8), df_keras_x_test.CHROM_a.getItem(9), df_keras_x_test.CHROM_a.getItem(10), df_keras_x_test.CHROM_a.getItem(11),  \\\n            df_keras_x_test.CHROM_a.getItem(12), df_keras_x_test.CHROM_a.getItem(13), df_keras_x_test.CHROM_a.getItem(14), df_keras_x_test.CHROM_a.getItem(15), df_keras_x_test.CHROM_a.getItem(16), df_keras_x_test.CHROM_a.getItem(17), \\\n            df_keras_x_test.CHROM_a.getItem(18), df_keras_x_test.CHROM_a.getItem(19), df_keras_x_test.CHROM_a.getItem(20), df_keras_x_test.CHROM_a.getItem(21), df_keras_x_test.CHROM_a.getItem(22), \\\n            df_keras_x_test.IMPACT_a.getItem(0), df_keras_x_test.IMPACT_a.getItem(1), df_keras_x_test.IMPACT_a.getItem(2),  \\\n            df_keras_x_test.CLNVCV_a.getItem(0), df_keras_x_test.CLNVCV_a.getItem(1), df_keras_x_test.CLNVCV_a.getItem(2), df_keras_x_test.CLNVCV_a.getItem(3), df_keras_x_test.CLNVCV_a.getItem(4), df_keras_x_test.CLNVCV_a.getItem(4),  \\\n            df_keras_x_test.SIFT_a.getItem(0), df_keras_x_test.SIFT_a.getItem(1), df_keras_x_test.SIFT_a.getItem(2), df_keras_x_test.SIFT_a.getItem(3),  \\\n            df_keras_x_test.BLOSUM62_a.getItem(0),\n            \"PolyPhen\").drop(\"cadd_a\").drop(\"freqs_a\").drop(\"CHROM_a\").drop(\"IMPACT_a\").drop(\"CLNVCV_a\").drop(\"SIFT_a\").drop(\"BLOSUM62_norm\")\n            "
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- CLASS: integer (nullable = true)\n |-- STRAND: integer (nullable = true)\n |-- LoFtool: double (nullable = true)\n |-- ORIGIN_LOG2: double (nullable = true)\n |-- cadd_a[0]: float (nullable = true)\n |-- cadd_a[1]: float (nullable = true)\n |-- freqs_a[0]: float (nullable = true)\n |-- freqs_a[1]: float (nullable = true)\n |-- freqs_a[2]: float (nullable = true)\n |-- CHROM_a[0]: float (nullable = true)\n |-- CHROM_a[1]: float (nullable = true)\n |-- CHROM_a[2]: float (nullable = true)\n |-- CHROM_a[3]: float (nullable = true)\n |-- CHROM_a[4]: float (nullable = true)\n |-- CHROM_a[5]: float (nullable = true)\n |-- CHROM_a[6]: float (nullable = true)\n |-- CHROM_a[7]: float (nullable = true)\n |-- CHROM_a[8]: float (nullable = true)\n |-- CHROM_a[9]: float (nullable = true)\n |-- CHROM_a[10]: float (nullable = true)\n |-- CHROM_a[11]: float (nullable = true)\n |-- CHROM_a[12]: float (nullable = true)\n |-- CHROM_a[13]: float (nullable = true)\n |-- CHROM_a[14]: float (nullable = true)\n |-- CHROM_a[15]: float (nullable = true)\n |-- CHROM_a[16]: float (nullable = true)\n |-- CHROM_a[17]: float (nullable = true)\n |-- CHROM_a[18]: float (nullable = true)\n |-- CHROM_a[19]: float (nullable = true)\n |-- CHROM_a[20]: float (nullable = true)\n |-- CHROM_a[21]: float (nullable = true)\n |-- CHROM_a[22]: float (nullable = true)\n |-- IMPACT_a[0]: float (nullable = true)\n |-- IMPACT_a[1]: float (nullable = true)\n |-- IMPACT_a[2]: float (nullable = true)\n |-- CLNVCV_a[0]: float (nullable = true)\n |-- CLNVCV_a[1]: float (nullable = true)\n |-- CLNVCV_a[2]: float (nullable = true)\n |-- CLNVCV_a[3]: float (nullable = true)\n |-- CLNVCV_a[4]: float (nullable = true)\n |-- CLNVCV_a[4]: float (nullable = true)\n |-- SIFT_a[0]: float (nullable = true)\n |-- SIFT_a[1]: float (nullable = true)\n |-- SIFT_a[2]: float (nullable = true)\n |-- SIFT_a[3]: float (nullable = true)\n |-- BLOSUM62_a[0]: float (nullable = true)\n |-- PolyPhen: string (nullable = true)\n\n"
                }
            ], 
            "source": "df_keras_prep_train.printSchema()"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+-----+\n|         PolyPhen|count|\n+-----------------+-----+\n|           benign| 3463|\n|possibly_damaging| 3539|\n|probably_damaging| 3536|\n+-----------------+-----+\n\n+-----------------+-----+\n|         PolyPhen|count|\n+-----------------+-----+\n|           benign|  398|\n|possibly_damaging|  393|\n|probably_damaging|  413|\n+-----------------+-----+\n\n"
                }
            ], 
            "source": "df_keras_prep_train.groupBy(\"PolyPhen\").count().show()\n\ndf_keras_prep_test.groupBy(\"PolyPhen\").count().show()"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------------+-----+\n|PolyPhen_cat|count|\n+------------+-----+\n|           0| 3463|\n|           1| 3539|\n|           2| 3536|\n+------------+-----+\n\n+------------+-----+\n|PolyPhen_cat|count|\n+------------+-----+\n|           0|  398|\n|           1|  393|\n|           2|  413|\n+------------+-----+\n\n"
                }
            ], 
            "source": "df_keras_prep_cat_train = df_keras_prep_train.select(\"*\", F.when(df_keras_prep_train.PolyPhen == 'benign', 0) \\\n                     .when(df_keras_prep_train.PolyPhen == 'possibly_damaging', 1).when(df_keras_prep_train.PolyPhen == 'probably_damaging', 2).otherwise(-1).alias(\"PolyPhen_cat\")) \\\n                    .drop(\"PolyPhen\").select(\"*\")\n\ndf_keras_prep_cat_test = df_keras_prep_test.select(\"*\", F.when(df_keras_prep_test.PolyPhen == 'benign', 0) \\\n                     .when(df_keras_prep_test.PolyPhen == 'possibly_damaging', 1).when(df_keras_prep_test.PolyPhen == 'probably_damaging', 2).otherwise(-1).alias(\"PolyPhen_cat\")) \\\n                    .drop(\"PolyPhen\").select(\"*\")\n\n\n\ndf_keras_prep_cat_train.groupBy(\"PolyPhen_cat\").count().show()\ndf_keras_prep_cat_test.groupBy(\"PolyPhen_cat\").count().show()\n"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- CLASS: integer (nullable = true)\n |-- STRAND: integer (nullable = true)\n |-- LoFtool: double (nullable = true)\n |-- ORIGIN_LOG2: double (nullable = true)\n |-- cadd_a[0]: float (nullable = true)\n |-- cadd_a[1]: float (nullable = true)\n |-- freqs_a[0]: float (nullable = true)\n |-- freqs_a[1]: float (nullable = true)\n |-- freqs_a[2]: float (nullable = true)\n |-- CHROM_a[0]: float (nullable = true)\n |-- CHROM_a[1]: float (nullable = true)\n |-- CHROM_a[2]: float (nullable = true)\n |-- CHROM_a[3]: float (nullable = true)\n |-- CHROM_a[4]: float (nullable = true)\n |-- CHROM_a[5]: float (nullable = true)\n |-- CHROM_a[6]: float (nullable = true)\n |-- CHROM_a[7]: float (nullable = true)\n |-- CHROM_a[8]: float (nullable = true)\n |-- CHROM_a[9]: float (nullable = true)\n |-- CHROM_a[10]: float (nullable = true)\n |-- CHROM_a[11]: float (nullable = true)\n |-- CHROM_a[12]: float (nullable = true)\n |-- CHROM_a[13]: float (nullable = true)\n |-- CHROM_a[14]: float (nullable = true)\n |-- CHROM_a[15]: float (nullable = true)\n |-- CHROM_a[16]: float (nullable = true)\n |-- CHROM_a[17]: float (nullable = true)\n |-- CHROM_a[18]: float (nullable = true)\n |-- CHROM_a[19]: float (nullable = true)\n |-- CHROM_a[20]: float (nullable = true)\n |-- CHROM_a[21]: float (nullable = true)\n |-- CHROM_a[22]: float (nullable = true)\n |-- IMPACT_a[0]: float (nullable = true)\n |-- IMPACT_a[1]: float (nullable = true)\n |-- IMPACT_a[2]: float (nullable = true)\n |-- CLNVCV_a[0]: float (nullable = true)\n |-- CLNVCV_a[1]: float (nullable = true)\n |-- CLNVCV_a[2]: float (nullable = true)\n |-- CLNVCV_a[3]: float (nullable = true)\n |-- CLNVCV_a[4]: float (nullable = true)\n |-- CLNVCV_a[4]: float (nullable = true)\n |-- SIFT_a[0]: float (nullable = true)\n |-- SIFT_a[1]: float (nullable = true)\n |-- SIFT_a[2]: float (nullable = true)\n |-- SIFT_a[3]: float (nullable = true)\n |-- BLOSUM62_a[0]: float (nullable = true)\n |-- PolyPhen_cat: integer (nullable = false)\n\n"
                }
            ], 
            "source": "df_keras_prep_cat_train.printSchema()"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_train_keras = df_keras_prep_cat_train\ndf_test_keras = df_keras_prep_cat_test"
        }, 
        {
            "source": "## Keras Deep NN Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Current:\t 2.1.3\nExpected:\t 2.1.3\n"
                }
            ], 
            "source": "import keras\nprint('Current:\\t', keras.__version__)\nprint('Expected:\\t 2.1.3')"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Current:\t 1.5.0\nExpected:\t 1.5.0\n"
                }
            ], 
            "source": "import tensorflow as tf\nprint('Current:\\t', tf.__version__)\nprint('Expected:\\t 1.5.0')"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install keras==2.1.3"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install tensorflow==1.5.0"
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#some learners constantly reported 502 errors in Watson Studio. \n#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n#This is a workaround to limit resource consumption\n\nfrom keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))"
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10538 1204\n"
                }
            ], 
            "source": "print(df_train_keras.count(), df_test_keras.count())\n"
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_classes = 3"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pandas_data = df_train_keras.toPandas()\nx_train = np.array( pandas_data.iloc[:, :46] )\ny_train_0 = np.array( pandas_data.iloc[:, 46] ) \ny_train = keras.utils.to_categorical(y_train_0, num_classes)\n"
        }, 
        {
            "execution_count": 42, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pandas_data = df_test_keras.toPandas()\nx_test = np.array( pandas_data.iloc[:, :46] )\ny_test_0 = np.array( pandas_data.iloc[:, 46] )\ny_test = keras.utils.to_categorical(y_test_0, num_classes)"
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.models import load_model\n\nbatch_size = 128\nepochs = 1\n\ninput_width = 46\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "best_optimizer = ''\nbest_layer_size = 0\nbest_accuracy = 0\nbest_activation_fist_layer = ''\n\nprint('Lay.size: \\tOptimizer: \\tActiv_L1: \\tLoss: \\tAccuracy:')\n\nfor layer_size in range(32, 80, 1):\n    for optimizer in ['adagrad','adadelta','rmsprop']:\n        for activation_fist_layer in ['sigmoid','tanh','relu']:\n    \n            kmodel = Sequential()\n            kmodel.add(Dense(layer_size, activation=activation_fist_layer, input_shape=(input_width,)))\n            kmodel.add(Dropout(0.1))\n            kmodel.add(Dense(layer_size, activation = 'sigmoid'))\n            kmodel.add(Dropout(0.1))\n            kmodel.add(Dense(num_classes, activation='softmax'))\n\n            #kmodel.summary()\n\n            kmodel.compile(loss='categorical_crossentropy',\n                          optimizer=optimizer,\n                    metrics=['accuracy'])\n\n            kmodel.fit(x_train, y_train, batch_size=batch_size,\n                  epochs=epochs, validation_data=(x_test, y_test), verbose=0)\n\n            score = kmodel.evaluate(x_test, y_test, verbose=0)\n\n            print('%d \\t %s \\t %s \\t %f \\t %f' % (layer_size, optimizer[0:4], activation_fist_layer[0:4], score[0], score[1]))\n\n            if score[1]>best_accuracy:\n                best_optimizer = optimizer\n                best_layer_size = layer_size\n                best_activation_fist_layer = activation_fist_layer\n                best_loss = score[0]\n                best_accuracy = score[1]\n                print(\"*\")\n                kmodel.save('best_model.h5')\n            \n\nprint(\"\\nBest results:\")\nprint('Layer size: %d \\tOptimizer: %s \\tActivation_L1: %s \\tLoss: %f \\tAccuracy: %f' % (best_layer_size, best_optimizer, best_activation_fist_layer, best_loss, best_accuracy))\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Layer size: 71 \tOptimizer: adadelta \tActivation_L1: relu \tAccuracy: 0.757624\nbatch_size = 128\nbest_layer_size = 60\nbest_optimizer='adagrad'\nbest_activation_fist_layer='sigmoid'\nepochs=10\n\n\nprint(\"\\nNow retraining with best parameters...\")\nkmodel = Sequential()\nkmodel.add(Dense(best_layer_size, activation=best_activation_fist_layer, input_shape=(input_width,)))\nkmodel.add(Dropout(0.1))\n#kmodel.add(Dense(best_layer_size, activation = 'sigmoid'))\n#kmodel.add(Dropout(0.1))\nkmodel.add(Dense(num_classes, activation='softmax'))\n\nkmodel.compile(loss='categorical_crossentropy', optimizer=best_optimizer,\n                     metrics=['accuracy'])\n\nkmodel.fit(x_train, y_train, batch_size=batch_size,\n              epochs=epochs, validation_data=(x_test, y_test), verbose=0)\n\nkprediction = kmodel.predict(x_test)\n\nprint(\"Done predicting\")\n"
        }, 
        {
            "execution_count": 44, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "kmodel = load_model(\"best_model_iter_32_80.h5\")"
        }, 
        {
            "execution_count": 45, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 45, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([0, 0, 0, ..., 2, 2, 2])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_test_0"
        }, 
        {
            "execution_count": 46, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Classification report for NN\n             precision    recall  f1-score   support\n\n          0       0.75      0.75      0.75       398\n          1       0.49      0.46      0.47       393\n          2       0.66      0.70      0.68       413\n\navg / total       0.63      0.64      0.64      1204\n\n"
                }
            ], 
            "source": "yp = kmodel.predict_classes(x_test)\nyl  = y_test_0\n\nprint(\"Classification report for NN\")\nprint(classification_report(yl, yp))"
        }, 
        {
            "source": "### Deploy the best model to Watson Machine Learning", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "analyticslib.py\t\t  cleaned_data.csv\t\t       myfile.py\r\nbest_model_1.h5\t\t  cleaned_data_p.parquet\t       __pycache__\r\nbest_model_capstone.tgz   created_features_p_balanced.parquet  test.csv\r\nbest_model.h5\t\t  created_features_p.parquet\r\nbest_model_iter_32_80.h5  created_features_p.parquet.20190330\r\n"
                }
            ], 
            "source": "!ls\n#!mv best_model.h5 best_model_iter_32_80.h5"
        }, 
        {
            "execution_count": 48, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n#Best results:\n#Layer size: 71 \tOptimizer: adadelta \tActivation_L1: relu \tAccuracy: 0.757624\n\n\n#Best results:\n#Layer size: 63 \tOptimizer: adagrad \tActivation_L1: tanh \tAccuracy: 0.756832"
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "best_model_iter_32_80.h5\r\n"
                }
            ], 
            "source": "!tar -zcvf best_model_capstone.tgz best_model_iter_32_80.h5"
        }, 
        {
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n2019-03-31 00:49:17,711 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n"
                }
            ], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "wml_credentials = {\n  \"apikey\": \"VHXlht1bZAoI4b1a8FBZ2gw40ahwxRONHEg25_K680Wi\",\n  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/d582e46f99874f968a56be7438ead24b:386d8470-8340-4c25-a848-e6a3e2c3dc3b::\",\n  \"iam_apikey_name\": \"auto-generated-apikey-3df6a2a7-71e5-4d4c-94a6-e6aee0a7ea54\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/d582e46f99874f968a56be7438ead24b::serviceid:ServiceId-b9514503-3c54-445a-b1ee-74eb47e4ddff\",\n  \"instance_id\": \"386d8470-8340-4c25-a848-e6a3e2c3dc3b\",\n  \"password\": \"fe53b083-986c-42e2-b704-e30d615aa4f1\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"3df6a2a7-71e5-4d4c-94a6-e6aee0a7ea54\"\n}"
        }, 
        {
            "execution_count": 52, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)"
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_props = {\n    client.repository.ModelMetaNames.AUTHOR_NAME: \"FernandoLopez\",\n    client.repository.ModelMetaNames.AUTHOR_EMAIL: \"superserious.supernatural@gmail.com\",\n    client.repository.ModelMetaNames.NAME: \"KerasCapstone\",\n    client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n    client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.5\",\n    client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{\"name\":\"keras\", \"version\":\"2.1.3\"}]    \n}"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "published_model = client.repository.store_model(model=\"best_model_capstone.tgz\", meta_props=model_props)"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "------------------------------------  ------------------  ------  --------------  ------------------------  --------------  -------------\nGUID                                  NAME                TYPE    STATE           CREATED                   FRAMEWORK       ARTIFACT TYPE\n36ca412a-48a9-4041-9ace-1295061bec70  created_best_model  online  DEPLOY_SUCCESS  2019-03-31T03:27:49.860Z  tensorflow-1.5  model\n------------------------------------  ------------------  ------  --------------  ------------------------  --------------  -------------\n"
                }
            ], 
            "source": "client.deployments.list()"
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "published_model_uid = client.repository.get_model_uid(published_model)"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_details = client.repository.get_details(published_model_uid)"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 59, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'entity': {'author': {'name': 'FernandoLopez'},\n  'deployments': {'count': 0,\n   'url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190/deployments'},\n  'evaluation_metrics_url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190/evaluation_metrics',\n  'feedback_url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190/feedback',\n  'latest_version': {'created_at': '2019-03-31T05:49:17.904Z',\n   'guid': '8372bac7-d889-4d46-98d5-a267fd2b5f1b',\n   'url': 'https://us-south.ml.cloud.ibm.com/v3/ml_assets/models/832f6252-a1cf-40f5-83ea-475d51696190/versions/8372bac7-d889-4d46-98d5-a267fd2b5f1b'},\n  'learning_configuration_url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190/learning_configuration',\n  'learning_iterations_url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190/learning_iterations',\n  'model_type': 'tensorflow-1.5',\n  'name': 'KerasCapstone',\n  'runtime_environment': 'None Provided'},\n 'metadata': {'created_at': '2019-03-31T05:49:17.844Z',\n  'guid': '832f6252-a1cf-40f5-83ea-475d51696190',\n  'modified_at': '2019-03-31T05:49:17.904Z',\n  'url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/published_models/832f6252-a1cf-40f5-83ea-475d51696190'}}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model_details"
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '832f6252-a1cf-40f5-83ea-475d51696190' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_IN_PROGRESS\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='0d9a4359-e6d6-45fd-b83e-12fc0befb4ab'\n------------------------------------------------------------------------------------------------\n\n\n"
                }
            ], 
            "source": "created_deployment = client.deployments.create(published_model_uid, name=\"created_best_model\")"
        }, 
        {
            "source": "### Test deployed model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 61, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "https://us-south.ml.cloud.ibm.com/v3/wml_instances/386d8470-8340-4c25-a848-e6a3e2c3dc3b/deployments/0d9a4359-e6d6-45fd-b83e-12fc0befb4ab/online\n"
                }
            ], 
            "source": "scoring_endpoint = created_deployment['entity']['scoring_url']\nprint(scoring_endpoint)"
        }, 
        {
            "source": "PolyPhen coded severities:\n- 0 = benign\n- 1 = possibly damaging\n- 2 = probably damaging\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x_score_1 = x_test[10].tolist()"
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "This submission:\n"
                }, 
                {
                    "execution_count": 75, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0.0,\n -1.0,\n 0.984,\n 0.0,\n 0.02800000086426735,\n -0.8826540112495422,\n 0.01510000042617321,\n 0.0,\n 0.014999999664723873,\n 0.0,\n 0.0,\n 0.0,\n 1.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 1.0,\n 0.0,\n 0.0,\n 1.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 1.0,\n 0.0,\n 0.0]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "scoring_payload = {'values': [x_score_1]}\n\nprint('This submission:')\nx_score_1"
        }, 
        {
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Answer for this submission should be:  0\n"
                }
            ], 
            "source": "print('Answer for this submission should be: ', np.argmax(y_test[10]))\n"
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "...answer is: 0\n"
                }
            ], 
            "source": "predictions = client.deployments.score(scoring_endpoint, scoring_payload)\nprint('...answer is:', predictions['values'][0][1])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}